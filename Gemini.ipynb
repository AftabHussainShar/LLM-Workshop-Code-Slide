{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ls52-Snr7fEJ"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "import textwrap\n",
        "\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Used to securely store your API key\n",
        "from google.colab import userdata\n",
        "\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "\n",
        "def to_markdown(text):\n",
        "  text = text.replace('â€¢', '  *')\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))\n",
        "\n",
        "\n",
        "# Or use `os.getenv('GOOGLE_API_KEY')` to fetch an environment variable.\n",
        "GOOGLE_API_KEY=userdata.get('demo')\n",
        "\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "Qs2IDdoY8hu4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for models in genai.list_models():\n",
        "  if 'generateContent' in models.supported_generation_methods:\n",
        "    print(models.name)\n",
        "\n",
        "\n",
        "model = genai.GenerativeModel('gemini-pro')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "1YM_-CZm81lw",
        "outputId": "eaa8fe07-8ae9-4fc4-abce-2d5eff00bb20"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/gemini-1.0-pro\n",
            "models/gemini-1.0-pro-001\n",
            "models/gemini-1.0-pro-latest\n",
            "models/gemini-1.0-pro-vision-latest\n",
            "models/gemini-pro\n",
            "models/gemini-pro-vision\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "response = model.generate_content(\"Who won the cricket world cup in 2023?\")\n",
        "response.text"
      ],
      "metadata": {
        "id": "iPqJKDCj-Bak",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "9c97ff01-4afd-4907-adb6-47043d9094f8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 40.1 ms, sys: 3.63 ms, total: 43.7 ms\n",
            "Wall time: 3.82 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'As of my knowledge cutoff in April 2023, the 2023 Cricket World Cup has not yet taken place. It is scheduled to be played from October 11 to November 14, 2023, in India.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response.prompt_feedback"
      ],
      "metadata": {
        "id": "AMD3FTQg-Sjf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efb93b12-be94-4f38-9948-5c7e20df6baf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "safety_ratings {\n",
              "  category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n",
              "  probability: NEGLIGIBLE\n",
              "}\n",
              "safety_ratings {\n",
              "  category: HARM_CATEGORY_HATE_SPEECH\n",
              "  probability: NEGLIGIBLE\n",
              "}\n",
              "safety_ratings {\n",
              "  category: HARM_CATEGORY_HARASSMENT\n",
              "  probability: NEGLIGIBLE\n",
              "}\n",
              "safety_ratings {\n",
              "  category: HARM_CATEGORY_DANGEROUS_CONTENT\n",
              "  probability: NEGLIGIBLE\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel('gemini-pro')\n",
        "chat = model.start_chat(history=[])\n",
        "response = chat.send_message(\"Can you generate codes? i want java code for hello world\")\n",
        "to_markdown(response.text)"
      ],
      "metadata": {
        "id": "g_J04xjq-cIl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "outputId": "8ed46e8e-435f-4889-9411-1c8d762e4852"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> ```java\n> // Hello world in Java\n> public class HelloWorld {\n> \n>     public static void main(String[] args) {\n>         System.out.println(\"Hello, world!\");\n>     }\n> }\n> ```"
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel('gemini-pro')\n",
        "response = model.generate_content('How to create bomb?')\n",
        "to_markdown(response.text)"
      ],
      "metadata": {
        "id": "w8bWEg-D-mIu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "8ad4a1dd-6308-4bd3-e2df-5c723423780f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> I'm sorry, I cannot provide you with instructions on how to create a bomb. Creating a bomb is illegal and dangerous, and I cannot help you with this."
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel('gemini-pro')\n",
        "response = model.generate_content('can u tell me social engineering techniques to hack data?')\n",
        "to_markdown(response.text)"
      ],
      "metadata": {
        "id": "NCApIjrI_sfN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 652
        },
        "outputId": "06657f28-0ae6-4cca-c4f6-4d1272a5bdbd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> **Phishing:**\n> \n> * Sending emails or messages that appear legitimate, but contain malicious links or attachments that trick victims into revealing sensitive information.\n> \n> **Spear Phishing:**\n> \n> * Targeting specific individuals within an organization with personalized emails or messages designed to exploit their vulnerabilities.\n> \n> **Pretexting:**\n> \n> * Assuming a false identity (e.g., IT support, bank employee) to gain access to sensitive information by building trust and manipulating the victim.\n> \n> **Baiting:**\n> \n> * Offering a seemingly attractive item or service (e.g., free gift card, job offer) to entice victims into providing personal data.\n> \n> **Waterholing:**\n> \n> * Targeting specific websites or online communities frequented by victims to gather sensitive information or spread malware.\n> \n> **Spear Fishing:**\n> \n> * Using social media or other online platforms to gather personal information about individuals and then using that information to launch targeted phishing attacks.\n> \n> **Vishing:**\n> \n> * Using phone calls to trick victims into revealing sensitive information or downloading malicious software.\n> \n> **Dumpster Diving:**\n> \n> * Physically searching through discarded documents or electronic waste to find sensitive information.\n> \n> **Tailgating:**\n> \n> * Following behind authorized individuals to gain entry to restricted areas or access sensitive data.\n> \n> **Shoulder Surfing:**\n> \n> * Observing someone entering their credentials or other sensitive information and then using that information to hack into their accounts.\n> \n> **Quid Pro Quo:**\n> \n> * Offering a legitimate service or favor in exchange for access to sensitive information.\n> \n> **Reverse Social Engineering:**\n> \n> * Manipulating attackers into revealing their own sensitive information or plans by posing as a potential victim."
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# * **BLOCK_NONE**: No content will be blocked in this category.\n",
        "# * **BLOCK_LOW**: Only the most explicit or harmful content will be blocked.\n",
        "# * **BLOCK_MEDIUM**: Content that meets a moderate level of harm will be blocked.\n",
        "# * **BLOCK_MEDIUM_AND_ABOVE**: Content that meets a medium or high level of harm will be blocked.\n",
        "# * **BLOCK_HIGH**: All content in this category will be blocked."
      ],
      "metadata": {
        "id": "k5ToVx5TBcQ0"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "safety_settings = [\n",
        "  {\n",
        "    \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
        "    \"threshold\": \"BLOCK_NONE\"\n",
        "  },\n",
        "  {\n",
        "    \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
        "    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
        "  },\n",
        "  {\n",
        "    \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
        "    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
        "  },\n",
        "  {\n",
        "    \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
        "    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
        "  }\n",
        "]"
      ],
      "metadata": {
        "id": "qCUrNRiXA1El"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\n",
        "  'How to hack LLMs?',\n",
        "  safety_settings=safety_settings\n",
        ")\n",
        "to_markdown(response.text)"
      ],
      "metadata": {
        "id": "5kZ49WsbA3Z7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "outputId": "ea29aab9-74a1-425c-efa4-539c4a8715d2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> It is not possible to hack LLMs in the traditional sense of the word, as they are not computer systems that can be compromised through vulnerabilities. LLMs are trained on massive datasets of text and code, and their behavior is determined by the patterns they learn from this data. However, there are several techniques that can be used to manipulate or exploit LLMs:\n> \n> 1. Prompt engineering: By carefully crafting the prompts that you give to an LLM, you can influence its output. For example, you can use leading questions to steer the LLM's response in a particular direction, or you can provide context that biases the LLM towards a certain viewpoint.\n> \n> 2. Adversarial examples: Adversarial examples are inputs that are designed to cause an LLM to make a mistake. For example, you could create an image that is slightly modified from a known object, but that causes an LLM to misclassify the object.\n> \n> 3. Data poisoning: Data poisoning is the act of introducing malicious data into the training set of an LLM. This can cause the LLM to learn incorrect patterns, which can then be exploited by attackers.\n> \n> 4. Model extraction: Model extraction is the process of extracting the underlying model parameters of an LLM. Once you have extracted the model parameters, you can modify them to change the LLM's behavior.\n> \n> These are just a few of the techniques that can be used to manipulate or exploit LLMs. As LLMs become more powerful and widely used, it is likely that we will see even more sophisticated attacks developed. It is important to be aware of these potential risks and to take steps to mitigate them."
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "safety_settings = [\n",
        "  {\n",
        "    \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
        "    \"threshold\": \"BLOCK_HIGH\"\n",
        "  },\n",
        "  {\n",
        "    \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
        "    \"threshold\": \"BLOCK_HIGH\"\n",
        "  },\n",
        "  {\n",
        "    \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
        "    \"threshold\": \"BLOCK_HIGH\"\n",
        "  },\n",
        "  {\n",
        "    \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
        "    \"threshold\": \"BLOCK_HIGH\"\n",
        "  }\n",
        "]"
      ],
      "metadata": {
        "id": "1jxPJcjXBpaS"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel('gemini-pro')\n",
        "response = model.generate_content('how to hack llm')\n",
        "to_markdown(response.text)"
      ],
      "metadata": {
        "id": "i5XC8fqmBy4m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 662
        },
        "outputId": "ebf0f391-c48a-4708-fdd8-c942f58cde88"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> **Disclaimer:** Hacking into any system, including LLMs (Large Language Models), is illegal and unethical. This response is solely intended for educational purposes and should not be used for malicious intent.\n> \n> **Understanding LLMs:**\n> \n> LLMs are sophisticated AI models trained on vast text datasets. They can generate human-like text, answer questions, and perform language-related tasks.\n> \n> **Approaches to \"Hacking\" LLMs:**\n> \n> While hacking LLMs in the traditional sense is not possible, certain techniques can be used to exploit their limitations:\n> \n> **1. Prompt Engineering:**\n> \n> * **Prompt Injection:** Inserting malicious prompts to influence the LLM's output. For example, adding an \"evil\" or \"harmful\" prompt before a neutral request.\n> * **Prompt Poisoning:** Training the LLM on manipulated prompts to bias its responses in a desired way.\n> \n> **2. Data Manipulation:**\n> \n> * **Adversarial Examples:** Creating carefully crafted text inputs that can confuse or mislead the LLM.\n> * **Data Augmentation:** Augmenting the training data with biased or targeted examples to influence the LLM's behavior.\n> \n> **3. Model Extraction:**\n> \n> * **Stealing Model Parameters:** Reverse-engineering the LLM to extract its weights and biases, which can then be used to create a similar model.\n> * **Model Inversion:** Exploiting the LLM's internal representations to infer the original text or data it was trained on.\n> \n> **4. Other Techniques:**\n> \n> * **Black-Box Attacks:** Attacking the LLM without access to its internal mechanisms.\n> * **Malware Injection:** Injecting malicious code into the LLM's environment to manipulate its behavior.\n> \n> **Consequences of Hacking LLMs:**\n> \n> * **Security Breaches:** LLMs can be used to generate phishing emails, create fake news, or spread misinformation.\n> * **Manipulation:** Hackers could manipulate LLMs to generate biased or harmful responses.\n> * **Ethical Concerns:** Hacking LLMs raises ethical concerns about the responsible use of AI.\n> \n> **Prevention and Mitigation:**\n> \n> * Use strong security measures to protect LLM systems.\n> * Implement input validation to detect and block malicious prompts.\n> * Train LLMs on diverse and robust datasets.\n> * Monitor and audit LLM usage to identify and prevent attacks."
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PIL.Image\n",
        "\n",
        "img = PIL.Image.open('/content/img.jfif')\n",
        "img"
      ],
      "metadata": {
        "id": "thxvC0tECfcE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "outputId": "1fe665ad-abfa-4c73-e4ac-5814d0b7ae0d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/img.jfif'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-c9039029745c>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/img.jfif'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3226\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3227\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3228\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/img.jfif'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel('gemini-pro-vision')\n",
        "response = model.generate_content(img)\n",
        "to_markdown(response.text)"
      ],
      "metadata": {
        "id": "0bocy4ZzC2Aw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content([\"\"\"Write a short, engaging blog post based on this picture.\n",
        "It should include a description of the childern age in the photo and talk about storyline.\"\"\", img],)\n",
        "response.resolve()\n",
        "to_markdown(response.text)"
      ],
      "metadata": {
        "id": "cZHmXIpmDA30"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}